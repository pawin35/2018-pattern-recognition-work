{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks with Keras ##\n",
    "\n",
    "In this exercise, you are going to build a set of deep learning models on a real world task using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google, and Keras is a frontend library built on top of Tensorflow (or Theano, CNTK) to provide an easier way to use standard layers and networks.\n",
    "\n",
    "To complete this exercise, you will need to build deep learning models for precipitation nowcasting. You will build a subset of the models shown below:\n",
    "- Fully Connected (Feedforward) Neural Network\n",
    "- Two-Dimentional Convolution Neural Network (2D-CNN)\n",
    "- Recurrent Neural Network with Gated Recurrent Unit (GRU)\n",
    "\n",
    "and one more model of your choice to achieve the highest score possible.\n",
    "\n",
    "We provide the code for data cleaning and some starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras). Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
    "\n",
    "This notebook assumes you have already installed Tensorflow and Keras with python3 and had GPU enabled. If you run this exercise on GCloud using the provided disk image you are all set.\n",
    "\n",
    "As a reminder,\n",
    "\n",
    "### Don't forget to shut down your instance on Gcloud when you are not using it ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precipitation Nowcasting ##\n",
    "\n",
    "Precipitation nowcasting is the the task of predicting the amount of rainfall in a certain region given some kind of sensor data.  The term nowcasting refers to tasks that try to predict the current or near future conditions (within 6 hours). \n",
    "\n",
    "You will be given satellite images in 3 different bands covering a 5 by 5 region from different parts of Thailand. In other words, your input will be a 5x5x3 image. Your task is to predict the amount of rainfal in the center pixel. You will first do the prediction using just a simple fully-connected neural network that view each pixel as different input features.\n",
    "\n",
    "Since the your input is basically an image, we will then view the input as an image and apply CNN to do the prediction. Finally, we can also add a time component since weather prediction can benefit greatly using previous time frames. Each data point actually contain 5 time steps, so each input data point has a size of 5x5x5x3 (time x height x width x channel), and the output data has a size of 5 (time). You will use this time information when you work with RNNs.\n",
    "\n",
    "Finally, we would like to thank the Thai Meteorological Department for providing the data for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explanation #\n",
    "\n",
    "The data is an hourly measurement of water vapor in the atmosphere, and two infrared measurements of cloud imagery on a latitude-longitude coordinate. Each measurement is illustrated below as an image. These three features are included as different channels in your input data.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/burin-n/pattern-recognition/master/HW4/images/wvapor.png\" width=\"200\"> <img src=\"https://raw.githubusercontent.com/burin-n/pattern-recognition/master/HW4/images/cloud1.png\" width=\"200\"> <img src=\"https://raw.githubusercontent.com/burin-n/pattern-recognition/master/HW4/images/cloud2.png\" width=\"200\">\n",
    "\n",
    "We also provide the hourly precipitation (rainfall) records in the month of June, July, August, September, and October from weather stations spreaded around the country. A 5x5 grid around each weather station at a particular time will be paired with the precipitation recorded at the corresponding station as input and output data. Finally, five adjacent timesteps are stacked into one sequence.\n",
    "\n",
    "The month of June-August are provided as training data, while the months of September and October are used as validation and test sets, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(months, data_dir='dataset'):\n",
    "    features = np.array([], dtype=np.float32).reshape(0,5,5,5,3)\n",
    "    labels = np.array([], dtype=np.float32).reshape(0,5)\n",
    "    for m in months:\n",
    "        filename = 'features-m{}.pk'.format(m)\n",
    "        with open(os.path.join(data_dir,filename), 'rb') as file:\n",
    "            features_temp = pickle.load(file)\n",
    "        features = np.concatenate((features, features_temp), axis=0)\n",
    "        \n",
    "        filename = 'labels-m{}.pk'.format(m)\n",
    "        with open(os.path.join(data_dir,filename), 'rb') as file:\n",
    "            labels_temp = pickle.load(file)\n",
    "        labels = np.concatenate((labels, labels_temp), axis=0)\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (229548, 5, 5, 5, 3)\n",
      "y_train shape: (229548, 5) \n",
      "\n",
      "x_val shape: (92839, 5, 5, 5, 3)\n",
      "y_val shape: (92839, 5) \n",
      "\n",
      "x_test shape: (111715, 5, 5, 5, 3)\n",
      "y_test shape: (111715, 5)\n"
     ]
    }
   ],
   "source": [
    "# use data from month 6,7,8 as training set\n",
    "x_train, y_train = read_data(months=[6,7,8])\n",
    "\n",
    "# use data from month 9 as validation set\n",
    "x_val, y_val = read_data(months=[9])\n",
    "\n",
    "# use data from month 10 as test set\n",
    "x_test, y_test = read_data(months=[10])\n",
    "\n",
    "print('x_train shape:',x_train.shape)\n",
    "print('y_train shape:', y_train.shape, '\\n')\n",
    "print('x_val shape:',x_val.shape)\n",
    "print('y_val shape:', y_val.shape, '\\n')\n",
    "print('x_test shape:',x_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**features** \n",
    "- dim 0: number of entries\n",
    "- dim 1: number of time-steps in ascending order\n",
    "- dim 2,3: a 5x5 grid around rain-measued station\n",
    "- dim 4: water vapor and two cloud imagenaries \n",
    "\n",
    "**labels**\n",
    "- dim 0: number of entries\n",
    "- dim 1: number of precipitation for each time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    mean = np.mean(X)\n",
    "    var = np.var(X)\n",
    "    return (X - mean) / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train)\n",
    "x_val = normalize(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-Layer Feedforward Neural Networks\n",
    "\n",
    "Below, the code for creating a 3-layers fully connected neural network in keras is provided. Run the code and make sure you understand what you are doing. Then, report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1147740, 75) (1147740, 1)\n",
      "(464195, 75) (464195, 1)\n"
     ]
    }
   ],
   "source": [
    "# Dataset need to be reshaped to make it suitable for feedforword model\n",
    "def preprocess_for_ff(x_train, y_train, x_val, y_val):\n",
    "    x_train_ff = x_train.reshape((-1, 5*5*3))\n",
    "    y_train_ff = y_train.reshape((-1, 1))\n",
    "    x_val_ff = x_val.reshape((-1, 5*5*3))\n",
    "    y_val_ff = y_val.reshape((-1, 1))\n",
    "    return x_train_ff, y_train_ff, x_val_ff, y_val_ff\n",
    "\n",
    "x_train_ff, y_train_ff, x_val_ff, y_val_ff = preprocess_for_ff(x_train, y_train, x_val, y_val)\n",
    "print(x_train_ff.shape, y_train_ff.shape)\n",
    "print(x_val_ff.shape, y_val_ff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def get_feedforward_nn():    \n",
    "    input1 = Input(shape=(75,))    \n",
    "    x = Dense(200, activation='relu')(input1)    \n",
    "    x = Dense(200, activation='relu')(x)\n",
    "    x = Dense(200, activation='relu')(x)\n",
    "    out = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "                loss='mse',\n",
    "                metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               15200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 95,801\n",
      "Trainable params: 95,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "# This is called to clear the original model session in order to use TensorBoard\n",
    "K.clear_session()\n",
    "\n",
    "model_ff = get_feedforward_nn()\n",
    "model_ff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ff\n",
      "Train on 1147740 samples, validate on 464195 samples\n",
      "Epoch 1/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9197 - mean_squared_error: 1.9197 - val_loss: 1.6588 - val_mean_squared_error: 1.6588\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65876, saving model to model_ff_nn.h5\n",
      "Epoch 2/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9186 - mean_squared_error: 1.9186 - val_loss: 1.6605 - val_mean_squared_error: 1.6605\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.65876\n",
      "Epoch 3/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9183 - mean_squared_error: 1.9183 - val_loss: 1.6623 - val_mean_squared_error: 1.6623\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.65876\n",
      "Epoch 4/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9177 - mean_squared_error: 1.9177 - val_loss: 1.6631 - val_mean_squared_error: 1.6631\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.65876\n",
      "Epoch 5/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9176 - mean_squared_error: 1.9176 - val_loss: 1.6595 - val_mean_squared_error: 1.6595\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.65876\n",
      "Epoch 6/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9175 - mean_squared_error: 1.9175 - val_loss: 1.6610 - val_mean_squared_error: 1.6610\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.65876\n",
      "Epoch 7/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9174 - mean_squared_error: 1.9174 - val_loss: 1.6619 - val_mean_squared_error: 1.6619\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.65876\n",
      "Epoch 8/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9175 - mean_squared_error: 1.9175 - val_loss: 1.6606 - val_mean_squared_error: 1.6606\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.65876\n",
      "Epoch 9/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9174 - mean_squared_error: 1.9174 - val_loss: 1.6616 - val_mean_squared_error: 1.6616\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.65876\n",
      "Epoch 10/10\n",
      "1147740/1147740 [==============================] - 7s 6us/step - loss: 1.9174 - mean_squared_error: 1.9174 - val_loss: 1.6611 - val_mean_squared_error: 1.6611\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.65876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5025179e10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "print('start training ff')\n",
    "\n",
    "# Path to save model parameters\n",
    "weight_path_model_ff ='model_ff_nn.h5'\n",
    "# Path to write tensorboard\n",
    "tensorboard_path_model_ff = 'Graphs/ff_nn'\n",
    "\n",
    "callbacks_list_model_ff_nn = [\n",
    "#     TensorBoard(log_dir=tensorboard_path_model_ff, histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "            weight_path_model_ff,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "verbose = 1\n",
    "epochs, batch_size = [10,1024]\n",
    "\n",
    "model_ff.fit(x_train_ff, y_train_ff, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "                callbacks=callbacks_list_model_ff_nn, validation_data=(x_val_ff, y_val_ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464195/464195 [==============================] - 14s 31us/step\n",
      "Mean square error of feedforward: 1.6587646545394852\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO#1:                                                                      #\n",
    "# Write a function to evaluate your model. Your function must make prediction  #\n",
    "# using the input model and return mean square error of the model.             #\n",
    "#                                                                              #\n",
    "# Hint: https://keras.io/models/model#evaluate                                 #\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "def evaluate(features, labels, model):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation data\n",
    "    \"\"\"\n",
    "    mse = model.evaluate(features, labels)[1]\n",
    "    return mse\n",
    "\n",
    "model_ff.save_weights('model_ff_nn_last.h5')\n",
    "model_ff.load_weights('model_ff_nn.h5')\n",
    "mse_ff = evaluate(x_val_ff, y_val_ff, model_ff)\n",
    "print(\"Mean square error of feedforward:\", mse_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use majority rule as a baseline.\n",
    "def majority_baseline(label_set):\n",
    "    unique, counts = np.unique(label_set, return_counts=True)\n",
    "    majority = unique[np.argmax(counts)]\n",
    "    baseline = 0\n",
    "    label_set = label_set.reshape(-1,1)\n",
    "    for r in label_set:\n",
    "        baseline += (majority - r) ** 2 / len(label_set)\n",
    "    pass\n",
    "    return baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "train [1.94397725]\n",
      "validate [1.6746546]\n"
     ]
    }
   ],
   "source": [
    "print('baseline')\n",
    "print('train', majority_baseline(y_train))\n",
    "print('validate', majority_baseline(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Tensorboard #\n",
    "The code provided also have Tensorboard (a visualization tool that comes with Tensorflow). Note the part that calls it `TensorBoard(log_dir='./Graph/' + graph_name, histogram_freq=1, write_graph=True, write_grads=True)`. This tells Tensorflow to write extra outputs to the `log_dir` which can then be used for visualization.\n",
    "\n",
    "To start tensorboard do\n",
    "```\n",
    "tensorboard --logdir=/full_path_to_your_logs\n",
    "```\n",
    "from the commandline. This will launch tensorboard, you will be able to access it from a web browser by pointint the url to `<instance-ip>:6006`. You will need to enable additional firewall rules in Gcloud for this.\n",
    "\n",
    "** Make sure your logs path is in the second drive (under /data). Otherwise, your main disk will be full! **\n",
    "\n",
    "In Tensorboard, you will be able to debug your computation graph which can be hard to keep track in code. This is might seem trivial in Keras, but it is very helpful for Tensorflow. You can see a visualization of the computation graph at the `GRAPH` tab. If you see multiple dense layers (more than 4), this is caused by running the code several times without deleting the log dir. Delete the log dir and re-run the code.\n",
    "\n",
    "Next, let's look at the scalars tab, we can see the loss and accuracy on the training and validation set as they change over each epoch. This can be useful to detect overfitting.\n",
    "\n",
    "Another useful tab is the histograms tab. This plot histograms of the weights, biases, and outputs of each layer. The depth of the histograms show the change over epochs. We can see how the histograms of weights change over the training peroid. This can be used to debug vanishing gradients or getting stuck in local minimas.\n",
    "\n",
    "There are other useful tabs in Tensorboard, you can read about them in the Keras [documentation](https://keras.io/callbacks/#tensorboard) for tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard observation #\n",
    "\n",
    "**Optional TODO#1** Write your own interpretation of the logs from this example. A simple sentence or two for each tab is sufficient.\n",
    "\n",
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout #\n",
    "\n",
    "You might notice that the 3-layered feedforward does not use dropout at all. Now, try adding dropout to the model, run, and report the result again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fully_connected_with_dropout():    \n",
    "    input1 = Input(shape=(75,))    \n",
    "    x = Dropout(0.2)(input1)    \n",
    "    x = Dense(200, activation='relu')(x)    \n",
    "    x = Dropout(0.2)(x)    \n",
    "    x = Dense(200, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)    \n",
    "    x = Dense(200, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)    \n",
    "    out = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(0.005),\n",
    "                loss='mse',\n",
    "                metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 75)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               15200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 95,801\n",
      "Trainable params: 95,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model_ff_dropout = get_fully_connected_with_dropout()\n",
    "model_ff_dropout.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO#2** Train you model with dropout below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training ff dropout\n",
      "Train on 1147740 samples, validate on 464195 samples\n",
      "Epoch 1/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9207 - mean_squared_error: 1.9207 - val_loss: 1.6607 - val_mean_squared_error: 1.6607\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66075, saving model to model_ff_nn_dropout.h5\n",
      "Epoch 2/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9202 - mean_squared_error: 1.9202 - val_loss: 1.6606 - val_mean_squared_error: 1.6606\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66075 to 1.66065, saving model to model_ff_nn_dropout.h5\n",
      "Epoch 3/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9199 - mean_squared_error: 1.9199 - val_loss: 1.6622 - val_mean_squared_error: 1.6622\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.66065\n",
      "Epoch 4/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9200 - mean_squared_error: 1.9200 - val_loss: 1.6600 - val_mean_squared_error: 1.6600\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.66065 to 1.65997, saving model to model_ff_nn_dropout.h5\n",
      "Epoch 5/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9198 - mean_squared_error: 1.9198 - val_loss: 1.6613 - val_mean_squared_error: 1.6613\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.65997\n",
      "Epoch 6/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9200 - mean_squared_error: 1.9200 - val_loss: 1.6603 - val_mean_squared_error: 1.6603\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.65997\n",
      "Epoch 7/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9189 - mean_squared_error: 1.9189 - val_loss: 1.6601 - val_mean_squared_error: 1.6601\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.65997\n",
      "Epoch 8/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9190 - mean_squared_error: 1.9190 - val_loss: 1.6617 - val_mean_squared_error: 1.6617\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.65997\n",
      "Epoch 9/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9187 - mean_squared_error: 1.9187 - val_loss: 1.6607 - val_mean_squared_error: 1.6607\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.65997\n",
      "Epoch 10/10\n",
      "1147740/1147740 [==============================] - 8s 7us/step - loss: 1.9187 - mean_squared_error: 1.9187 - val_loss: 1.6608 - val_mean_squared_error: 1.6608\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.65997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f502449eb00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO#3:                                                                      #\n",
    "# Complete the code to train your dropout model                                #\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "print('start training ff dropout')\n",
    "# Path to save model parameters\n",
    "weight_path_model_ff_dropout ='model_ff_nn_dropout.h5'\n",
    "# Path to write tensorboard\n",
    "tensorboard_path_model_ff_dropout = 'Graphs/ff_nn_dropout'\n",
    "\n",
    "callbacks_list_model_ff_nn_dropout = [\n",
    "#     TensorBoard(log_dir=tensorboard_path_model_ff_dropout, histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "            weight_path_model_ff_dropout,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "verbose = 1\n",
    "epochs, batch_size = [10,1024]\n",
    "\n",
    "model_ff_dropout.fit(x_train_ff, y_train_ff, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "                callbacks=callbacks_list_model_ff_nn_dropout, validation_data=(x_val_ff, y_val_ff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464195/464195 [==============================] - 15s 33us/step\n",
      "Mean square error of model with dropout: 1.6599748849463718\n",
      "The non dropout - dropout difference is: -0.0012102304068866143\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO#4:                                                                      #\n",
    "# Complete the code to evaluate your dropout model                             #\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "model_ff_dropout.save_weights('model_ff_nn_dropout_last.h5')\n",
    "model_ff_dropout.load_weights('model_ff_nn_dropout.h5')\n",
    "mse_ff_dropout = evaluate(x_val_ff, y_val_ff, model_ff_dropout)\n",
    "print(\"Mean square error of model with dropout:\", mse_ff_dropout)\n",
    "print(\"The non dropout - dropout difference is:\", mse_ff - mse_ff_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A fork on the road\n",
    "\n",
    "In the next Sections, we will discuss CNNs and GRUs. **PICK ONE** method to complete to finish the homework. If you do both methods, the other method counts as an optional task. Then, do the **Final Section**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks\n",
    "Now, you are going to implement you own 2d-convolution neural networks with the following structure.\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param\n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 5, 5, 3)           0         \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 3, 3, 200)         5600      \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 1800)              0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 200)               360200    \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 200)               40200     \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 1)                 201       \n",
    "=================================================================\n",
    "Total params: 406,201\n",
    "Trainable params: 406,201\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "These parameters are simple guidelines to save your time.    \n",
    "You can play with them in the final section which you can choose any normalization methods, activation function, as well as any hyperparameter the way you want.         \n",
    "\n",
    "Hint: You should read keras documentation to see the list of available layers and options you can use.                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1147740, 5, 5, 3) (1147740, 1)\n",
      "(464195, 5, 5, 3) (464195, 1)\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO#A1:                                                                     #\n",
    "# Complete the code for preparing data for training CNN                        #\n",
    "# Input for CNN should not have time step.                                     #\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "def preprocess_for_cnn(x_train, y_train, x_val, y_val):\n",
    "    x_train_cnn = x_train.reshape((-1, 5, 5, 3))\n",
    "    y_train_cnn = y_train.reshape((-1, 1))\n",
    "    x_val_cnn = x_val.reshape((-1, 5, 5, 3))\n",
    "    y_val_cnn = y_val.reshape((-1, 1))\n",
    "    return x_train_cnn, y_train_cnn, x_val_cnn, y_val_cnn\n",
    "\n",
    "x_train_cnn, y_train_cnn, x_val_cnn, y_val_cnn = preprocess_for_cnn(x_train, y_train, x_val, y_val)\n",
    "print(x_train_cnn.shape, y_train_cnn.shape)\n",
    "print(x_val_cnn.shape, y_val_cnn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    ################################################################################\n",
    "                    # TODO#A2:                                                                     #\n",
    "                    # Write a function that returns keras convolution nueral network model.        #\n",
    "                    ################################################################################\n",
    "                    #                            WRITE YOUR CODE BELOW                             #\n",
    "                    ################################################################################\n",
    "                    def get_conv2d_nn():\n",
    "                        input1 = Input(shape=(5,5,3,))    \n",
    "                        x = Conv2D(200, (3,3))(input1)\n",
    "                        x = Flatten()(x)\n",
    "                        x = Dense(200, activation='relu')(x)\n",
    "                        x = Dense(200, activation='relu')(x)\n",
    "                        out = Dense(1)(x)\n",
    "\n",
    "                        model = Model(inputs=input1, outputs=out)\n",
    "                        model.compile(optimizer=Adam(0.001),\n",
    "                                    loss='mse',\n",
    "                                    metrics=['mse'])\n",
    "\n",
    "                        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training conv2d\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 5, 5, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 3, 3, 200)         5600      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               360200    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 200)               40200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 406,201\n",
      "Trainable params: 406,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1147740 samples, validate on 464195 samples\n",
      "Epoch 1/10\n",
      " - 31s - loss: 1.9205 - mean_squared_error: 1.9205 - val_loss: 1.6601 - val_mean_squared_error: 1.6601\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66011, saving model to model_cnn_nn.h5\n",
      "Epoch 2/10\n",
      " - 30s - loss: 1.9195 - mean_squared_error: 1.9195 - val_loss: 1.6663 - val_mean_squared_error: 1.6663\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.66011\n",
      "Epoch 3/10\n",
      " - 30s - loss: 1.9192 - mean_squared_error: 1.9192 - val_loss: 1.6631 - val_mean_squared_error: 1.6631\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.66011\n",
      "Epoch 4/10\n",
      " - 30s - loss: 1.9183 - mean_squared_error: 1.9183 - val_loss: 1.6617 - val_mean_squared_error: 1.6617\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.66011\n",
      "Epoch 5/10\n",
      " - 31s - loss: 1.9180 - mean_squared_error: 1.9180 - val_loss: 1.6605 - val_mean_squared_error: 1.6605\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.66011\n",
      "Epoch 6/10\n",
      " - 31s - loss: 1.9178 - mean_squared_error: 1.9178 - val_loss: 1.6629 - val_mean_squared_error: 1.6629\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.66011\n",
      "Epoch 7/10\n",
      " - 30s - loss: 1.9177 - mean_squared_error: 1.9177 - val_loss: 1.6603 - val_mean_squared_error: 1.6603\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.66011\n",
      "Epoch 8/10\n",
      " - 30s - loss: 1.9177 - mean_squared_error: 1.9177 - val_loss: 1.6608 - val_mean_squared_error: 1.6608\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.66011\n",
      "Epoch 9/10\n",
      " - 30s - loss: 1.9176 - mean_squared_error: 1.9176 - val_loss: 1.6616 - val_mean_squared_error: 1.6616\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.66011\n",
      "Epoch 10/10\n",
      " - 30s - loss: 1.9176 - mean_squared_error: 1.9176 - val_loss: 1.6610 - val_mean_squared_error: 1.6610\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.66011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4fc408b048>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO#A3:                                                                     #\n",
    "# Write code that call model.fit, or model.fit_generator if you have data      #\n",
    "# generator, to train you models. Make sure you have validation_data as an     # \n",
    "# argument and use verbose=2 to generate one log line per epoch. Select your   #\n",
    "# batch size carefully as it will affect your model's ability to converge and  #\n",
    "# time needed for one epoch.                                                   #\n",
    "#                                                                              #\n",
    "# Hint: Read about callbacks_list argument on the documentation. You might     #\n",
    "# find  ReduceLROnPlateau() and ModelCheckpoint() useful for your training     #\n",
    "# process. Feel free to use any other callback function available.             #\n",
    "################################################################################\n",
    "print('start training conv2d')\n",
    "model_cnn = get_conv2d_nn()\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "model_cnn.summary()\n",
    "# Path to save model parameters\n",
    "weight_path_model_cnn ='model_cnn_nn.h5'\n",
    "# Path to write tensorboard\n",
    "tensorboard_path_model_cnn = 'Graphs/ff_nn'\n",
    "\n",
    "callbacks_list_model_cnn_nn = [\n",
    "#     TensorBoard(log_dir=tensorboard_path_model_cnn, histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "            weight_path_model_cnn,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "verbose = 2\n",
    "epochs, batch_size = [10,256]\n",
    "\n",
    "model_cnn.fit(x_train_cnn, y_train_cnn, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "                callbacks=callbacks_list_model_cnn_nn, validation_data=(x_val_cnn, y_val_cnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464195/464195 [==============================] - 22s 47us/step\n",
      "Mean square error of CNN: 1.6601089272342373\n",
      "Different from feedforward: -0.0013442726947521244\n",
      "Different from feedforward with dropout: -0.0001340422878655101\n"
     ]
    }
   ],
   "source": [
    "model_cnn.save_weights('model_cnn_nn_last.h5')\n",
    "model_cnn.load_weights('model_cnn_nn.h5')\n",
    "mse_cnn = evaluate(x_val_cnn, y_val_cnn, model_cnn)\n",
    "print(\"Mean square error of CNN:\", mse_cnn)\n",
    "print(\"Different from feedforward:\", mse_ff - mse_cnn)\n",
    "print(\"Different from feedforward with dropout:\", mse_ff_dropout - mse_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units\n",
    "\n",
    "Now, you are going to implement you own GRU network with the following structure.\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_1 (InputLayer)         (None, 5, 75)             0         \n",
    "_________________________________________________________________\n",
    "gru_1 (GRU)                  (None, 5, 200)            165600    \n",
    "_________________________________________________________________\n",
    "time_distributed_1 (TimeDist (None, 5, 200)            40200     \n",
    "_________________________________________________________________\n",
    "time_distributed_2 (TimeDist (None, 5, 1)              201       \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 5)                 0         \n",
    "=================================================================\n",
    "Total params: 206,001\n",
    "Trainable params: 206,001\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```\n",
    "\n",
    "\n",
    "These parameters are simple guidelines to save your time.    \n",
    "You can play with them in the final section which you can choose any normalization methods, activation function, as well as any hyperparameter the way you want.         \n",
    "The result should be better than the feedforward model and at least on par with your CNN model.    \n",
    "\n",
    "Do consult keras documentation on how to use [GRUs](https://keras.io/layers/recurrent/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(229548, 5, 75) (229548, 5)\n",
      "(92839, 5, 75) (92839, 5)\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO#B1:                                                                     #\n",
    "# Complete the code for preparing data for training GRU                        #\n",
    "# GRU's input should has 3 dimensions.                                         #\n",
    "# The dimensions should compose of entries, time-step, and features.          #\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "def preprocess_for_gru(x_train, y_train, x_val, y_val):\n",
    "    x_train_gru = x_train.reshape(-1,5,75)\n",
    "    x_val_gru = x_val.reshape(-1,5, 75)\n",
    "    y_train_gru = y_train.reshape(-1, 5)\n",
    "    y_val_gru = y_val.reshape(-1,5)\n",
    "    return x_train_gru, y_train_gru, x_val_gru, y_val_gru\n",
    "x_train_gru, y_train_gru, x_val_gru, y_val_gru = preprocess_for_gru(x_train, y_train, x_val, y_val)\n",
    "print(x_train_gru.shape, y_train_gru.shape)\n",
    "print(x_val_gru.shape, y_val_gru.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# TODO#B2                                                                      #\n",
    "# Write a function that returns keras GRU network model.                       #\n",
    "# Your goal is to predict a precipitation of every time step.                  #\n",
    "#                                                                              #\n",
    "# Hint: You should read keras documentation to see the list of available       #\n",
    "# layers and options you can use.                                              #\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "\n",
    "def get_gru():    \n",
    "    input1 = Input(shape=(5,75,))    \n",
    "    x = GRU(200,return_sequences=True)(input1)\n",
    "    x = TimeDistributed(Dense(200))(x)\n",
    "    x = TimeDistributed(Dense(1))(x)\n",
    "    out = Flatten()(x)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(0.0005),\n",
    "                loss='mse',\n",
    "                metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training gru\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 5, 75)             0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 5, 200)            165600    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 200)            40200     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 1)              201       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 206,001\n",
      "Trainable params: 206,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 229548 samples, validate on 92839 samples\n",
      "Epoch 1/10\n",
      " - 9s - loss: 1.9182 - mean_squared_error: 1.9182 - val_loss: 1.6637 - val_mean_squared_error: 1.6637\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66365, saving model to model_gru_nn.h5\n",
      "Epoch 2/10\n",
      " - 8s - loss: 1.9158 - mean_squared_error: 1.9158 - val_loss: 1.6607 - val_mean_squared_error: 1.6607\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66365 to 1.66074, saving model to model_gru_nn.h5\n",
      "Epoch 3/10\n",
      " - 8s - loss: 1.9150 - mean_squared_error: 1.9150 - val_loss: 1.6568 - val_mean_squared_error: 1.6568\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.66074 to 1.65684, saving model to model_gru_nn.h5\n",
      "Epoch 4/10\n",
      " - 8s - loss: 1.9145 - mean_squared_error: 1.9145 - val_loss: 1.6599 - val_mean_squared_error: 1.6599\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.65684\n",
      "Epoch 5/10\n",
      " - 8s - loss: 1.9139 - mean_squared_error: 1.9139 - val_loss: 1.6636 - val_mean_squared_error: 1.6636\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.65684\n",
      "Epoch 6/10\n",
      " - 8s - loss: 1.9129 - mean_squared_error: 1.9129 - val_loss: 1.6604 - val_mean_squared_error: 1.6604\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.65684\n",
      "Epoch 7/10\n",
      " - 8s - loss: 1.9127 - mean_squared_error: 1.9127 - val_loss: 1.6594 - val_mean_squared_error: 1.6594\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.65684\n",
      "Epoch 8/10\n",
      " - 8s - loss: 1.9126 - mean_squared_error: 1.9126 - val_loss: 1.6580 - val_mean_squared_error: 1.6580\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.65684\n",
      "Epoch 9/10\n",
      " - 8s - loss: 1.9126 - mean_squared_error: 1.9126 - val_loss: 1.6591 - val_mean_squared_error: 1.6591\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.65684\n",
      "Epoch 10/10\n",
      " - 8s - loss: 1.9124 - mean_squared_error: 1.9124 - val_loss: 1.6593 - val_mean_squared_error: 1.6593\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.65684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4faa0d9860>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO#B3                                                                      #\n",
    "# Write code that call model.fit, or model.fit_generator if you have data      #\n",
    "# generator, to train you models. Make sure you have validation_data as an     # \n",
    "# argument and use verbose=2 to generate one log line per epoch. Select your   #\n",
    "# batch size carefully as it will affect your model's ability to converge and  #\n",
    "# time needed for one epoch.                                                   #\n",
    "#                                                                              #\n",
    "# Hint: Read about callbacks_list argument on the documentation. You might     #\n",
    "# find  ReduceLROnPlateau() and ModelCheckpoint() useful for your training     #\n",
    "# process. Feel free to use any other callback function available.             #\n",
    "################################################################################\n",
    "print('start training gru')\n",
    "model_gru = get_gru()\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "model_gru.summary()\n",
    "# Path to save model parameters\n",
    "weight_path_model_gru ='model_gru_nn.h5'\n",
    "# Path to write tensorboard\n",
    "tensorboard_path_model_gru = 'Graphs/ff_nn'\n",
    "\n",
    "callbacks_list_model_gru_nn = [\n",
    "#     TensorBoard(log_dir=tensorboard_path_model_gru, histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "            weight_path_model_gru,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "verbose = 2\n",
    "epochs, batch_size = [10,512]\n",
    "\n",
    "model_gru.fit(x_train_gru, y_train_gru, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "                callbacks=callbacks_list_model_gru_nn, validation_data=(x_val_gru, y_val_gru))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92839/92839 [==============================] - 15s 158us/step\n",
      "Mean square error of gru: 1.6568418235959599\n",
      "Different from CNN: 0.0032671036382774243\n",
      "Different from feedforward: 0.0019228309435253\n",
      "Different from feedforward with dropout: 0.0031330613504119142\n"
     ]
    }
   ],
   "source": [
    "model_gru.save_weights('model_gru_nn_last.h5')\n",
    "model_gru.load_weights('model_gru_nn.h5')\n",
    "mse_gru = evaluate(x_val_gru, y_val_gru, model_gru)\n",
    "print(\"Mean square error of gru:\", mse_gru)\n",
    "print(\"Different from CNN:\", mse_cnn - mse_gru)\n",
    "print(\"Different from feedforward:\", mse_ff - mse_gru)\n",
    "print(\"Different from feedforward with dropout:\", mse_ff_dropout - mse_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Section\n",
    "# Keras playground\n",
    "\n",
    "Now, train the best model you can do for this task. You can use any model structure and function available.    \n",
    "Remember that trainig time increases with the complexity of the model. You might find TensorBoard helpful in tuning of complicated models.    \n",
    "Your model should be better than your CNN or GRU model in the previous sections.\n",
    "\n",
    "You should tune your model on training and validation set.    \n",
    "**The test set should be used only for the last evaluation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# TODO#5                                                                       #\n",
    "# Write a function that returns keras your best model. You can use anything    #\n",
    "# you want. The goal here is to create the best model you can think of.        #\n",
    "#                                                                              #\n",
    "# Hint: You should read keras documentation to see the list of available       #\n",
    "# layers and options you can use.                                              #\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "\n",
    "def get_my_best_model():\n",
    "    input1 = Input(shape=(5,75,))    \n",
    "    x = LSTM(200,return_sequences=True)(input1)\n",
    "    x = LSTM(200,return_sequences=True)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = TimeDistributed(Dense(200, activation='relu'))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = TimeDistributed(Dense(200, activation='relu'))(x)\n",
    "    x = TimeDistributed(Dense(1))(x)\n",
    "    out = Flatten()(x)\n",
    "\n",
    "    model = Model(inputs=input1, outputs=out)\n",
    "    model.compile(optimizer=Adam(0.0005),\n",
    "                loss='mse',\n",
    "                metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training the best model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 5, 75)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5, 200)            220800    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 200)            320800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 5, 200)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 5, 200)            40200     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5, 200)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 5, 200)            40200     \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 5, 1)              201       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 622,201\n",
      "Trainable params: 622,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 229548 samples, validate on 92839 samples\n",
      "Epoch 1/50\n",
      " - 33s - loss: 1.9169 - mean_squared_error: 1.9169 - val_loss: 1.6571 - val_mean_squared_error: 1.6571\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.65709, saving model to model_best_nn.h5\n",
      "Epoch 2/50\n",
      " - 31s - loss: 1.9147 - mean_squared_error: 1.9147 - val_loss: 1.6580 - val_mean_squared_error: 1.6580\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.65709\n",
      "Epoch 3/50\n",
      " - 31s - loss: 1.9141 - mean_squared_error: 1.9141 - val_loss: 1.6583 - val_mean_squared_error: 1.6583\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.65709\n",
      "Epoch 4/50\n",
      " - 31s - loss: 1.9135 - mean_squared_error: 1.9135 - val_loss: 1.6585 - val_mean_squared_error: 1.6585\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.65709\n",
      "Epoch 5/50\n",
      " - 32s - loss: 1.9131 - mean_squared_error: 1.9131 - val_loss: 1.6566 - val_mean_squared_error: 1.6566\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.65709 to 1.65658, saving model to model_best_nn.h5\n",
      "Epoch 6/50\n",
      " - 32s - loss: 1.9129 - mean_squared_error: 1.9129 - val_loss: 1.6568 - val_mean_squared_error: 1.6568\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.65658\n",
      "Epoch 7/50\n",
      " - 32s - loss: 1.9131 - mean_squared_error: 1.9131 - val_loss: 1.6637 - val_mean_squared_error: 1.6637\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.65658\n",
      "Epoch 8/50\n",
      " - 31s - loss: 1.9125 - mean_squared_error: 1.9125 - val_loss: 1.6599 - val_mean_squared_error: 1.6599\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.65658\n",
      "Epoch 9/50\n",
      " - 30s - loss: 1.9124 - mean_squared_error: 1.9124 - val_loss: 1.6611 - val_mean_squared_error: 1.6611\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.65658\n",
      "Epoch 10/50\n",
      " - 30s - loss: 1.9122 - mean_squared_error: 1.9122 - val_loss: 1.6580 - val_mean_squared_error: 1.6580\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.65658\n",
      "Epoch 11/50\n",
      " - 31s - loss: 1.9121 - mean_squared_error: 1.9121 - val_loss: 1.6596 - val_mean_squared_error: 1.6596\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.65658\n",
      "Epoch 12/50\n",
      " - 32s - loss: 1.9119 - mean_squared_error: 1.9119 - val_loss: 1.6587 - val_mean_squared_error: 1.6587\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.65658\n",
      "Epoch 13/50\n",
      " - 30s - loss: 1.9119 - mean_squared_error: 1.9119 - val_loss: 1.6569 - val_mean_squared_error: 1.6569\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.65658\n",
      "Epoch 14/50\n",
      " - 30s - loss: 1.9117 - mean_squared_error: 1.9117 - val_loss: 1.6574 - val_mean_squared_error: 1.6574\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.65658\n",
      "Epoch 15/50\n",
      " - 30s - loss: 1.9117 - mean_squared_error: 1.9117 - val_loss: 1.6589 - val_mean_squared_error: 1.6589\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.65658\n",
      "Epoch 16/50\n",
      " - 31s - loss: 1.9116 - mean_squared_error: 1.9116 - val_loss: 1.6566 - val_mean_squared_error: 1.6566\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.65658 to 1.65656, saving model to model_best_nn.h5\n",
      "Epoch 17/50\n",
      " - 31s - loss: 1.9116 - mean_squared_error: 1.9116 - val_loss: 1.6595 - val_mean_squared_error: 1.6595\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.65656\n",
      "Epoch 18/50\n",
      " - 31s - loss: 1.9115 - mean_squared_error: 1.9115 - val_loss: 1.6597 - val_mean_squared_error: 1.6597\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.65656\n",
      "Epoch 19/50\n",
      " - 30s - loss: 1.9115 - mean_squared_error: 1.9115 - val_loss: 1.6602 - val_mean_squared_error: 1.6602\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.65656\n",
      "Epoch 20/50\n",
      " - 31s - loss: 1.9113 - mean_squared_error: 1.9113 - val_loss: 1.6585 - val_mean_squared_error: 1.6585\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.65656\n",
      "Epoch 21/50\n",
      " - 31s - loss: 1.9112 - mean_squared_error: 1.9112 - val_loss: 1.6585 - val_mean_squared_error: 1.6585\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.65656\n",
      "Epoch 22/50\n",
      " - 31s - loss: 1.9111 - mean_squared_error: 1.9111 - val_loss: 1.6593 - val_mean_squared_error: 1.6593\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.65656\n",
      "Epoch 23/50\n",
      " - 32s - loss: 1.9111 - mean_squared_error: 1.9111 - val_loss: 1.6585 - val_mean_squared_error: 1.6585\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.65656\n",
      "Epoch 24/50\n",
      " - 32s - loss: 1.9109 - mean_squared_error: 1.9109 - val_loss: 1.6588 - val_mean_squared_error: 1.6588\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.65656\n",
      "Epoch 25/50\n",
      " - 32s - loss: 1.9110 - mean_squared_error: 1.9110 - val_loss: 1.6618 - val_mean_squared_error: 1.6618\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.65656\n",
      "Epoch 26/50\n",
      " - 32s - loss: 1.9109 - mean_squared_error: 1.9109 - val_loss: 1.6581 - val_mean_squared_error: 1.6581\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.65656\n",
      "Epoch 27/50\n",
      " - 31s - loss: 1.9108 - mean_squared_error: 1.9108 - val_loss: 1.6608 - val_mean_squared_error: 1.6608\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.65656\n",
      "Epoch 28/50\n",
      " - 31s - loss: 1.9108 - mean_squared_error: 1.9108 - val_loss: 1.6592 - val_mean_squared_error: 1.6592\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.65656\n",
      "Epoch 29/50\n",
      " - 31s - loss: 1.9107 - mean_squared_error: 1.9107 - val_loss: 1.6586 - val_mean_squared_error: 1.6586\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.65656\n",
      "Epoch 30/50\n",
      " - 31s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6599 - val_mean_squared_error: 1.6599\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.65656\n",
      "Epoch 31/50\n",
      " - 32s - loss: 1.9107 - mean_squared_error: 1.9107 - val_loss: 1.6597 - val_mean_squared_error: 1.6597\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.65656\n",
      "Epoch 32/50\n",
      " - 32s - loss: 1.9107 - mean_squared_error: 1.9107 - val_loss: 1.6603 - val_mean_squared_error: 1.6603\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.65656\n",
      "Epoch 33/50\n",
      " - 32s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6608 - val_mean_squared_error: 1.6608\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.65656\n",
      "Epoch 34/50\n",
      " - 31s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6613 - val_mean_squared_error: 1.6613\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.65656\n",
      "Epoch 35/50\n",
      " - 31s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6601 - val_mean_squared_error: 1.6601\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.65656\n",
      "Epoch 36/50\n",
      " - 30s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6600 - val_mean_squared_error: 1.6600\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.65656\n",
      "Epoch 37/50\n",
      " - 32s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6602 - val_mean_squared_error: 1.6602\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.65656\n",
      "Epoch 38/50\n",
      " - 32s - loss: 1.9105 - mean_squared_error: 1.9105 - val_loss: 1.6612 - val_mean_squared_error: 1.6612\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.65656\n",
      "Epoch 39/50\n",
      " - 32s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6612 - val_mean_squared_error: 1.6612\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.65656\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 31s - loss: 1.9104 - mean_squared_error: 1.9104 - val_loss: 1.6616 - val_mean_squared_error: 1.6616\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.65656\n",
      "Epoch 41/50\n",
      " - 31s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6605 - val_mean_squared_error: 1.6605\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.65656\n",
      "Epoch 42/50\n",
      " - 31s - loss: 1.9104 - mean_squared_error: 1.9104 - val_loss: 1.6595 - val_mean_squared_error: 1.6595\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.65656\n",
      "Epoch 43/50\n",
      " - 31s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6611 - val_mean_squared_error: 1.6611\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.65656\n",
      "Epoch 44/50\n",
      " - 31s - loss: 1.9105 - mean_squared_error: 1.9105 - val_loss: 1.6611 - val_mean_squared_error: 1.6611\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.65656\n",
      "Epoch 45/50\n",
      " - 31s - loss: 1.9104 - mean_squared_error: 1.9104 - val_loss: 1.6603 - val_mean_squared_error: 1.6603\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.65656\n",
      "Epoch 46/50\n",
      " - 31s - loss: 1.9106 - mean_squared_error: 1.9106 - val_loss: 1.6603 - val_mean_squared_error: 1.6603\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.65656\n",
      "Epoch 47/50\n",
      " - 32s - loss: 1.9105 - mean_squared_error: 1.9105 - val_loss: 1.6607 - val_mean_squared_error: 1.6607\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.65656\n",
      "Epoch 48/50\n",
      " - 31s - loss: 1.9104 - mean_squared_error: 1.9104 - val_loss: 1.6611 - val_mean_squared_error: 1.6611\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.65656\n",
      "Epoch 49/50\n",
      " - 31s - loss: 1.9105 - mean_squared_error: 1.9105 - val_loss: 1.6605 - val_mean_squared_error: 1.6605\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.65656\n",
      "Epoch 50/50\n",
      " - 31s - loss: 1.9105 - mean_squared_error: 1.9105 - val_loss: 1.6600 - val_mean_squared_error: 1.6600\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.65656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4f8dc666a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# TODO#6                                                                       #\n",
    "# Write code that call model.fit, or model.fit_generator if you have data      #\n",
    "# generator, to train you models. Make sure you have validation_data as an     # \n",
    "# argument and use verbose=2 to generate one log line per epoch. Select your   #\n",
    "# batch size carefully as it will affect your model's ability to converge and  #\n",
    "# time needed for one epoch.                                                   #\n",
    "#                                                                              #\n",
    "# Hint: Read about callbacks_list argument on the documentation. You might     #\n",
    "# find  ReduceLROnPlateau() and ModelCheckpoint() useful for your training     #\n",
    "# process. Feel free to use any other callback function available.             #\n",
    "################################################################################\n",
    "print('start training the best model')\n",
    "model_best = get_my_best_model()\n",
    "################################################################################\n",
    "#                            WRITE YOUR CODE BELOW                             #\n",
    "################################################################################\n",
    "x_train_best = x_train_gru.copy()\n",
    "y_train_best = y_train_gru.copy()\n",
    "x_val_best = x_val_gru.copy()\n",
    "y_val_best = y_val_gru.copy()\n",
    "model_best.summary()\n",
    "# Path to save model parameters\n",
    "weight_path_model_best ='model_best_nn.h5'\n",
    "# Path to write tensorboard\n",
    "tensorboard_path_model_best = 'Graphs/ff_nn'\n",
    "\n",
    "callbacks_list_model_best_nn = [\n",
    "#     TensorBoard(log_dir=tensorboard_path_model_best, histogram_freq=1, write_graph=True, write_grads=True),\n",
    "    ModelCheckpoint(\n",
    "            weight_path_model_best,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, min_lr=0.00001)\n",
    "]\n",
    "\n",
    "verbose = 2\n",
    "epochs, batch_size = [50,256]\n",
    "\n",
    "model_best.fit(x_train_best, y_train_best, epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "                callbacks=callbacks_list_model_best_nn, validation_data=(x_val_best, y_val_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92839/92839 [==============================] - 34s 361us/step\n",
      "Last model\n",
      "Mean square error of best: 1.6565593197344861\n",
      "Different from feedforward: 0.0022053348049990706\n",
      "Different from feedforward with dropout: 0.003415565211885685\n",
      "Different from CNN: 0.003549607499751195\n",
      "Different from GRU: 0.0002825038614737707\n",
      "Test set evaluate\n",
      "558575/558575 [==============================] - 30s 53us/step\n",
      "111715/111715 [==============================] - 19s 169us/step\n",
      "111715/111715 [==============================] - 41s 370us/step\n",
      "Test of CNN: 32782.73145409426\n",
      "Test of GRU: 2.128476027049198\n",
      "Test of best: 1.1619463834452906\n"
     ]
    }
   ],
   "source": [
    "model_best.save_weights('model_best_nn_last.h5')\n",
    "model_best.load_weights('model_best_nn.h5')\n",
    "x_train_cnn, y_train_cnn, x_test_cnn, y_test_cnn = preprocess_for_cnn(x_train, y_train, x_test, y_test)\n",
    "x_train_gru, y_train_gru, x_test_gru, y_test_gru = preprocess_for_gru(x_train, y_train, x_test, y_test)\n",
    "mse_best = evaluate(x_val_best, y_val_best, model_best)\n",
    "print(\"Last model\")\n",
    "print(\"Mean square error of best:\", mse_best)\n",
    "print(\"Different from feedforward:\", mse_ff - mse_best)\n",
    "print(\"Different from feedforward with dropout:\", mse_ff_dropout - mse_best)\n",
    "print(\"Different from CNN:\", mse_cnn - mse_best)\n",
    "print(\"Different from GRU:\", mse_gru - mse_best)\n",
    "\n",
    "print(\"Test set evaluate\")\n",
    "test_cnn = evaluate(x_test_cnn, y_test_cnn, model_cnn)\n",
    "test_gru = evaluate(x_test_gru, y_test_gru, model_gru)\n",
    "test_best = evaluate(x_test_gru, y_test_gru, model_best)\n",
    "print(\"Test of CNN:\", test_cnn)\n",
    "print(\"Test of GRU:\", test_gru)\n",
    "print(\"Test of best:\", test_best)\n",
    "#Also evaluate your fully-connected model and CNN/GRU model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get full credit for this part, your best model should be better than the previous models on the **test set**. The top 5 students will recieve 2 additional points. The top student will recieve another 2 additional points on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
